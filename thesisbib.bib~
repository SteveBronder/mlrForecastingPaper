@book{adams1995hitchhiker,
  title={The Hitchhiker's Guide to the Galaxy},
  author={Adams, D.},
  isbn={9781417642595},
  url={http://books.google.com/books?id=W-xMPgAACAAJ},
  year={1995},
  publisher={San Val}
}

@TechReport{Hyndman2006,
  author={Rob J. Hyndman and Anne B. Koehler},
  title={{Another Look at Measures of Forecast Accuracy}},
  year=2005,
  month=May,
  institution={Monash University, Department of Econometrics and Business Statistics},
  type={Monash Econometrics and Business Statistics Working Papers},
  url={https://ideas.repec.org/p/msh/ebswps/2005-13.html},
  number={13/05},
  abstract={We discuss and compare measures of accuracy of univariate time series forecasts. The methods used in the M-competition and the M3-competition, and many of the measures recommended by previous authors on this topic, are found to be inadequate, and many of them are degenerate in commonly occurring situations. Instead, we propose that the mean absolute scaled error become the standard measure for comparing forecast accuracy across multiple time series.},
  keywords={Forecast accuracy; Forecast evaluation; Forecast error measures; M-competition; Mean absolute scaled},
  doi={},
}

@ARTICLE{Spyros1993,
title = {Accuracy measures: theoretical and practical concerns},
author = {Makridakis, Spyros},
year = {1993},
journal = {International Journal of Forecasting},
volume = {9},
number = {4},
pages = {527-529},
url = {http://EconPapers.repec.org/RePEc:eee:intfor:v:9:y:1993:i:4:p:527-529}
}

@ARTICLE{Armstrong1992,
title = {Error measures for generalizing about forecasting methods: Empirical comparisons},
author = {Armstrong, J. and Collopy, Fred},
year = {1992},
journal = {International Journal of Forecasting},
volume = {8},
number = {1},
pages = {69-80},
url = {http://EconPapers.repec.org/RePEc:eee:intfor:v:8:y:1992:i:1:p:69-80}
}

@article{kullback1951,
author = "Kullback, S. and Leibler, R. A.",
doi = "10.1214/aoms/1177729694",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "03",
number = "1",
pages = "79--86",
publisher = "The Institute of Mathematical Statistics",
title = "On Information and Sufficiency",
url = "http://dx.doi.org/10.1214/aoms/1177729694",
volume = "22",
year = "1951"
}

@article{THOMPSON1990,
title = "An MSE statistic for comparing forecast accuracy across series",
journal = "International Journal of Forecasting",
volume = "6",
number = "2",
pages = "219 - 227",
year = "1990",
note = "",
issn = "0169-2070",
doi = "http://dx.doi.org/10.1016/0169-2070(90)90007-X",
url = "http://www.sciencedirect.com/science/article/pii/016920709090007X",
author = "Patrick A. Thompson",
keywords = "Forecast accuracy",
keywords = "Forecasting competitions",
keywords = "Mean squared error"
}

@article{Lin1991,
 author = {Lin, J.},
 title = {Divergence Measures Based on the Shannon Entropy},
 journal = {IEEE Trans. Inf. Theor.},
 issue_date = {January 1991},
 volume = {37},
 number = {1},
 month = sep,
 year = {2006},
 issn = {0018-9448},
 pages = {145--151},
 numpages = {7},
 url = {http://dx.doi.org/10.1109/18.61115},
 doi = {10.1109/18.61115},
 acmid = {2267073},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
} 

@article{Ebrahimi1995,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3214930},
 abstract = {A new partial ordering among life distributions in terms of their uncertainties is introduced. Our measure of uncertainty is Shannon information applied to the residual lifetime. The relationship between this ordering and various existing orderings of life distributions are discussed. Various properties of our proposed concept are examined. Based on our proposed ordering and various existing orderings, the notion of a 'better system' is introduced.},
 author = {Nader Ebrahimi, Franco Pellerey},
 journal = {Journal of Applied Probability},
 number = {1},
 pages = {202-211},
 publisher = {Applied Probability Trust},
 title = {New Partial Ordering of Survival Functions Based on the Notion of Uncertainty},
 volume = {32},
 year = {1995}
}

@article{Kang2016,
title = "Some new results on the \{LQE\} ordering ",
journal = "Statistical Methodology ",
volume = "",
number = "",
pages = " - ",
year = "2016",
note = "",
issn = "1572-3127",
doi = "http://dx.doi.org/10.1016/j.stamet.2016.06.001",
url = "http://www.sciencedirect.com/science/article/pii/S1572312716300120",
author = "Dian-tong Kang",
keywords = "Residual entropy",
keywords = "Residual quantile entropy order",
keywords = "Closure and reversed closure property",
keywords = "Proportional odds model",
keywords = "Survival copula "
}

@article{Hyndman25,
title = "25 years of time series forecasting ",
journal = "International Journal of Forecasting ",
volume = "22",
number = "3",
pages = "443 - 473",
year = "2006",
note = "Twenty five years of forecasting ",
issn = "0169-2070",
doi = "http://dx.doi.org/10.1016/j.ijforecast.2006.01.001",
url = "http://www.sciencedirect.com/science/article/pii/S0169207006000021",
author = "Jan G. De Gooijer and Rob J. Hyndman",
keywords = "Accuracy measures",
keywords = "ARCH",
keywords = "ARIMA",
keywords = "Combining",
keywords = "Count data",
keywords = "Densities",
keywords = "Exponential smoothing",
keywords = "Kalman filter",
keywords = "Long memory",
keywords = "Multivariate",
keywords = "Neural nets",
keywords = "Nonlinearity",
keywords = "Prediction intervals",
keywords = "Regime-switching",
keywords = "Robustness",
keywords = "Seasonality",
keywords = "State space",
keywords = "Structural models",
keywords = "Transfer function",
keywords = "Univariate",
keywords = "VAR "
}

@Article{HyndForecast,
    title = {Automatic time series forecasting: the forecast package
      for {R}},
    author = {Rob J Hyndman and Yeasmin Khandakar},
    journal = {Journal of Statistical Software},
    volume = {26},
    number = {3},
    pages = {1--22},
    year = {2008},
    url = {http://www.jstatsoft.org/article/view/v027i03},
}

@Manual{Rbase,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2015},
    url = {https://www.R-project.org/},
}

@Manual{rugarch,
    title = {rugarch: Univariate GARCH models.},
    author = {Alexios Ghalanos},
    year = {2015},
    note = {R package version 1.3-6.},
}


@Manual{mlr,
    title = {mlr: Machine Learning in R},
    author = {Bernd Bischl and Michel Lang and Jakob Richter and Jakob Bossek and Leonard Judt and Tobias Kuehn and Erich Studerus and Lars Kotthoff},
    year = {2015},
    note = {R package version 2.7},
    url = {https://CRAN.R-project.org/package=mlr},
 }

@Manual{caret,
    title = {caret: Classification and Regression Training},
    author = {Max Kuhn. Contributions from Jed Wing and Steve Weston and Andre Williams and Chris Keefer and Allan Engelhardt and Tony Cooper and Zachary Mayer and Brenton Kenkel and the R Core Team and Michael Benesty and Reynald Lescarbeau and Andrew Ziem and Luca Scrucca and Yuan Tang and Can Candan.},
    year = {2015},
    note = {R package version 6.0-62},
    url = {https://CRAN.R-project.org/package=caret},
}

@article{Makridakis2000451,
title = "The M3-Competition: results, conclusions and implications ",
journal = "International Journal of Forecasting ",
volume = "16",
number = "4",
pages = "451 - 476",
year = "2000",
note = "The M3- Competition ",
issn = "0169-2070",
doi = "http://dx.doi.org/10.1016/S0169-2070(00)00057-1",
url = "http://www.sciencedirect.com/science/article/pii/S0169207000000571",
author = "Spyros Makridakis and Michèle Hibon",
keywords = "Comparative methods — time series: univariate",
keywords = "Forecasting competitions",
keywords = "M-Competition",
keywords = "Forecasting methods, Forecasting accuracy "
}

@Manual{m4comp,
    title = {M4comp: Data from the M4 Time Series Forecasting Competition},
    author = {Souhaib BenTaieb},
    year = {2016},
    note = {R package version 0.0.1},
    url = {https://CRAN.R-project.org/package=M4comp},
}

@article{MurphymeteoForecast,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2288395},
 author = {Allan H. Murphy, Robert L. Winkler},
 journal = {Journal of the American Statistical Association},
 number = {387},
 pages = {489-500},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Probability Forecasting in Meterology},
 volume = {79},
 year = {1984}
}

@article{Chadefaux01012014,
author = {Chadefaux, Thomas}, 
title = {Early warning signals for war in the news},
volume = {51}, 
number = {1}, 
pages = {5-18}, 
year = {2014}, 
doi = {10.1177/0022343313507302}, 
URL = {http://jpr.sagepub.com/content/51/1/5.abstract}, 
eprint = {http://jpr.sagepub.com/content/51/1/5.full.pdf+html}, 
journal = {Journal of Peace Research} 
}

@article{earthquakeYegu,
 ISSN = {00251909, 15265501},
 URL = {http://www.jstor.org/stable/2629612},
 abstract = {The primary concern of this paper is the establishment of a forecasting procedure for largest earthquake magnitudes of the next n years. It is shown that the use of the third-type asymptotic distribution of the largest values as an approximation to the exact distribution provides for an expression for the expected value and the mode of the largest earthquake magnitude in the next n years. An example of the application of this procedure is given and the results are compared with actual observations.},
 author = {Tuncel M. Yegulalp},
 journal = {Management Science},
 number = {4},
 pages = {418-421},
 publisher = {INFORMS},
 title = {Forecasting for Largest Earthquakes},
 volume = {21},
 year = {1974}
}


@book{TrippiNueralForecast,
 editor = {Trippi, Robert R. and Turban, Efraim},
 title = {Neural Networks in Finance and Investing: Using Artificial Intelligence to Improve Real World Performance},
 year = {1992},
 isbn = {1557384525},
 publisher = {McGraw-Hill, Inc.},
 address = {New York, NY, USA},
} 

@Article{Cao2001,
author="Cao, Lijuan
and Tay, E.H Francis",
title="Financial Forecasting Using Support Vector Machines",
journal="Neural Computing {\&} Applications",
year="2001",
volume="10",
number="2",
pages="184--192",
issn="1433-3058",
doi="10.1007/s005210170010",
url="http://dx.doi.org/10.1007/s005210170010"
}

@article{GRANGER19923,
title = "Forecasting stock market prices: Lessons for forecasters",
journal = "International Journal of Forecasting",
volume = "8",
number = "1",
pages = "3 - 13",
year = "1992",
note = "",
issn = "0169-2070",
doi = "http://dx.doi.org/10.1016/0169-2070(92)90003-R",
url = "http://www.sciencedirect.com/science/article/pii/016920709290003R",
author = "Clive W.J. Granger",
keywords = "Forecastability",
keywords = "Stock returns",
keywords = "Non-linear models",
keywords = "Efficient markets"
}


@book{hyndman2014forecasting,
  title={Forecasting: principles and practice: },
  author={Hyndman, R.J. and Athanasopoulos, G.},
  isbn={9780987507105},
  url={https://books.google.com/books?id=gDuRBAAAQBAJ},
  year={2014},
  publisher={OTexts}
}

@Manual{xts,
    title = {xts: eXtensible Time Series},
    author = {Jeffrey A. Ryan and Joshua M. Ulrich},
    year = {2016},
    note = {R package version 0.10-0},
    url = {https://github.com/joshuaulrich/xts},
}

@article{hannanOrder,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2335856},
 abstract = {The order, (p,q), of an autoregressive-moving average sequence, y(t), may be estimated by minimizing a criterion, logσ̂2 + (p + q)log T/T, with respect to p and q, where σ̂2 is the maximum likelihood estimate of the variance of the innovations, ε(t). It is suggested that, instead, σ2 be estimated from a series of regressions of y(t) on y(t - 1), y(t - 2),...,y(t - p), ε̂(t - 1),...,ε̂(t - q), where the ε̂(t) are obtained by fitting a long autoregression to the data. It is shown how the sequence of regressions may, for p = q, be economically recursively calculated by embedding them in a sequence of bivariate autoregressions. Asymptotic properties of the procedure are established under very general conditions.},
 author = {E. J. Hannan, J. Rissanen},
 journal = {Biometrika},
 number = {1},
 pages = {81-94},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Recursive Estimation of Mixed Autoregressive-Moving Average Order},
 volume = {69},
 year = {1982}
}

@article{forecastpro,
 author = {Goodrich RL},
 journal = {International Journal of Forecasting},
 number = {4},
 pages = {533-535},
 title = {The Forecast Pro Methodology},
 volume = {16},
 year = {2000}
}

@article{reillyautobox,
  title={The AUTOBOX system},
  author={Reilly, David},
  journal={International Journal of Forecasting},
  volume={16},
  number={4},
  pages={531--533},
  year={2000},
  publisher={Elsevier}
}

@online{windowingcaret,
  author = {Max Kuhn},
  title = {{caret} data splitting methods},
  year = 1999,
  url = {http://caret.r-forge.r-project.org/splitting.html},
  urldate = {2014-05-30}
}

@Article{Friedman1997,
author="Friedman, Jerome H.",
title="On Bias, Variance, 0/1---Loss, and the Curse-of-Dimensionality",
journal="Data Mining and Knowledge Discovery",
year="1997",
volume="1",
number="1",
pages="55--77",
abstract="The classification problem is considered in which an outputvariable y assumes discrete values with respectiveprobabilities that depend upon the simultaneous values of a set of input variablesx = {\{}x{\_}1,....,x{\_}n{\}}. At issue is how error in the estimates of theseprobabilities affects classification error when the estimates are used ina classification rule. These effects are seen to be somewhat counterintuitive in both their strength and nature. In particular the bias andvariance components of the estimation error combine to influenceclassification in a very different way than with squared error on theprobabilities themselves. Certain types of (very high) bias can becanceled by low variance to produce accurate classification. This candramatically mitigate the effect of the bias associated with some simpleestimators like ``naive'' Bayes, and the bias induced by thecurse-of-dimensionality on nearest-neighbor procedures. This helps explainwhy such simple methods are often competitive with and sometimes superiorto more sophisticated ones for classification, and why``bagging/aggregating'' classifiers can often improveaccuracy. These results also suggest simple modifications to theseprocedures that can (sometimes dramatically) further improve theirclassification performance.",
issn="1573-756X",
doi="10.1023/A:1009778005914",
url="http://dx.doi.org/10.1023/A:1009778005914"
}

@ARTICLE{rodriguezkfold,
author={J. D. Rodriguez and A. Perez and J. A. Lozano},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Sensitivity Analysis of k-Fold Cross Validation in Prediction Error Estimation},
year={2010},
volume={32},
number={3},
pages={569-575},
keywords={Bayes methods;estimation theory;learning (artificial intelligence);pattern classification;probability;statistical analysis;classification error estimator;k-fold cross-validation;machine learning;naive Bayes method;nearest neighbor algorithm;prediction error estimation;probability distribution;sensitivity analysis;statistical properties;bias and variance;decomposition of the variance;error estimation;k-fold cross validation;prediction error;sources of sensitivity;supervised classification.},
doi={10.1109/TPAMI.2009.187},
ISSN={0162-8828},
month={March}
}

@Manual{paramhelper,
    title = {ParamHelpers: Helpers for Parameters in Black-Box Optimization, Tuning and
Machine Learning},
    author = {Bernd Bischl and Michel Lang and Jakob Bossek and Daniel Horn and Jakob Richter and Pascal Kerschke},
    year = {2016},
    note = {R package version 1.9},
    url = {https://CRAN.R-project.org/package=ParamHelpers},
}



