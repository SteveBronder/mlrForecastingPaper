\documentclass{article}%[final]
\setcounter{secnumdepth}{3}
% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

%\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{nips_2016}
\usepackage{amsmath}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{float}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{color,soul}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[margin=1.5in]{geometry}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{longtable}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

% Code defs from jss
\newcommand\code{\@codex}
\def\@codex#1{{\normalfont\ttfamily\hyphenchar\font=-1 #1}}
%%\let\code=\texttt
\let\proglang=\textsf
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}


%\theoremstyle{definition}
%\newtheorem{definition}{Definition}[section]


\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\title{Time Series Methods in the R package \pkg{mlr}}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Steve Bronder \\
  %Quantitative Methods of the Social Sciences\\
  %Columbia University\\
  %New York City, NY 10027 \\
  \texttt{sab2287@columbia.edu} \\
  %% examples of more authors test etst
   %Department of Computer Science \\
   %Columbia University\\
   %New York City, NY 10027 \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
The \pkg{mlr} package is a unified interface for machine learning tasks such as classification, regression, cluster analysis, and survival analysis. \pkg{mlr} handles the data pipeline of pre-processing, resampling, model selection, model tuning, ensembling, and prediction. This paper details new methods for developing time series  models in \pkg{mlr}. It includes standard and novel tools such as auto-regressive and LambertW transform data generating processes, fixed and growing window cross validation, and forecasting models in the context of univariate and multivariate time series. Examples from forecasting competitions will be given in order to demonstrate the benefits of a unified framework for machine learning and time series.
  \end{abstract}

\section{Introduction}
There has been a rapid developement in time series methods over the last 25 years ~\cite{Hyndman25}. Time series models have not only become more common, but more complex. The \proglang{R} language ~\cite{Rbase} has a large task view with many packages available for forecasting and time series methods. However, without a standard framework, many packages have their own sub-culture of style, syntax, and output. The \pkg{mlr} ~\cite{mlr} package, short for Machine Learning in R, works to give a strong syntatic framework for the modeling pipeline. By automating many of the standard tools in machine learning such as preprocessing and cross validation, \pkg{mlr} reduces error in the modeling process that is derived from the user. 

While there are some time series methods available in \pkg{caret} ~\cite{caret}, development of forecasting models in \pkg{caret} is difficult due to computational constraints and design choices. The highly modular structure of \pkg{mlr} makes it the best choice for implementing time series methods and models. This paper will show how using \pkg{mlr}'s strong syntatic structure allows for time series packages such as \pkg{forecast} ~\cite{HyndForecast} and \pkg{rugarch} ~\cite{rugarch} to use machine learning methedologies such as automated parameter tuning, data preprocessing, model blending, cross validation, performance evaluation, and parallel processing techniques for decreasing model build time.

\section{Forecasting Example with the M4 Competition}
\label{sec:m4data}

Professional forecasters attempt to predict the future of a series based on its past values. Forecasting can be used in a wide range of tasks including forecasting stock prices, ~\cite{GRANGER19923}, weather patterns ~\cite{MurphymeteoForecast}, international conficts ~\cite{Chadefaux01012014}, and earthquakes ~\cite{earthquakeYegu}. In order to evaluate \pkg{mlr}'s forecasting framework we need a large set of possible time series to make sure our methods generalize well.\footnote{Very goofy sentence need to fix}

The Makridakis competitions ~\cite{Makridakis2000451} are forecasting challenges organized by the International Institute of Forecasters and led by Spyros Makridakis to evaluate and compare the accuracy of forecasting methods. The most recent of the competitions, the M4 competition, contains 10,000 time series on a yearly, quarterly, monthly, and daily frequency in areas such as finance, macroeconomics, climate, microeconomics, and industry. To show examples of how \pkg{mlr}'s forecasting features work we will look at a particular climate series. The data is daily with the training subset starting on September 6th, 2007 and ending on September 5th, 2009 while the testing subset is from September 6th, 2009 to October 10th, 2009 for a total of 640 training periods and 35 test periods to forecast.


%For our purposes we will look at two particular daily financial series, one with 9136 observations from April 10th, 1971 to April 13th, 1996 and another with 6742 observations from January 7th, 1981 to June 23rd, 1999. Each series must be forecast 328 and 242 periods into the future, respectively.



<<get_dat, cache = TRUE, fig.height= 4, fig.width=4, fig.align='center', echo = FALSE>>=
library(M4comp)
library(xts)
library(lubridate)
m4Fin1 <- M4[[8836]]
m4Train1 <- xts(m4Fin1$past, as.POSIXct("2007-12-05") + days(0:I(length(m4Fin1$past)-1)))
m4Test1 <- xts(m4Fin1$future, as.POSIXct("2009-09-06") + days(0:I(length(m4Fin1$future)-1)))
colnames(m4Train1) <- "target_var"
colnames(m4Test1) <- "target_var"

#m4Fin2 <- M4[[29]]
#m4Train2 <- xts(m4Fin2$past, as.POSIXct("1981-01-07") + days(0:I(length(m4Fin2$past)-1)))
#m4Test2 <- xts(m4Fin2$future, as.POSIXct("1999-06-23") + days(0:I(length(m4Fin2$future)-1)))
#colnames(m4Train2) <- "target_var"
#colnames(m4Test2) <- "target_var"
plot(m4Train1, main = "Daily Climate Data")
#plot(m4Train2, main = "Daily Financial Data Two")
@

%These two series were chosen for their large time features and stark contrast.\footnote{I think it would be better to just use one series for examples, and actually train / test over all of M4 later} Our data set should be large enough that the tuning method can take multiple windows of the data. Some series in M4 only contain 12 observations, which is not enough data to accurately train a model. These two time series were chosen as they are the two largest ones in the M4 competitions data set. We can see figure one is what most people imagine when they think of a time series. Figure two shows a series which appears to have a sort of step feature. The stark difference between the time process of the two series will allow us to investigate whether the methods in \pkg{mlr}'s forecasting framework can find the appropriate model. The data can be found in the package \pkg{M4comp} ~\cite{m4comp} under sets `M4[28]` and `M4[29]. 

\section{Forecasting Tasks}
\label{sec:task}

\pkg{mlr} provides uses the S3 object system to clearly define a predictive modeling task. Tasks contain the data and other relevant information such as the task id and which variable you are targeting for supervised learning problems. Forecasting tasks are handled in \pkg{mlr} by the function \code{makeForecastRegrTask()}. The forecasting task inherets from \code{makeRegrTask}, but has two noticable differences in parameters.

\begin{itemize}
\item[data:] Instead of a data frame, an xts object from \pkg{xts} ~\cite{xts} containing the time series.
\item[frequency:] An integer with the number of periods your time series contains. For example, daily data with a weekly periodicity has a frequency of 7 while daily data with a yearly periodicity has a frequency of 365.
\end{itemize}

<<fin_task, eval = TRUE>>=
library(mlr)
climate.task = makeForecastRegrTask(id = "M4 Climate Data",
                                 data = m4Train1,
                                 target = "target_var",
                                 frequency = 365L)
climate.task
@

\section{Building a forecast learner}

The \code{makeLearner()} function provides a structured model building framework to the 7 forecasting models currently implimented in \pkg{mlr}. As an example, we will build a simple AutoRegressive Integrated Moving Average (ARIMA) model. The ARIMA model is of the form

\begin{equation}
y_t \sim \alpha + \beta_1 \Delta_d y_{t-1} ... \beta_p \Delta_d y_{t-p} + \phi_1 \epsilon_{t-1} + ... + \phi_q \epsilon_{t-q} + \epsilon_t
\end{equation}

\begin{equation}
y_t \sim \alpha + \sum_{i=1}^p \beta_i \Delta_d y_{t-i} + \sum_{i=1}^q \phi_i \epsilon_{t-i} +\epsilon_t
\end{equation}

In equation three, $\alpha$ is a constant, $\beta_p$ is the coefficient associated with the lagged observations of $y$ with $\Delta_d$ being the $d$th difference operator. The coefficient for the one step forecast error $\epsilon_{t-q}$ is $\phi_q$. ARIMA is one of the most well known forecasting models and is avaible in mlr along with models such as BATS, TBATS, THIEF ~\cite{thief}, ETS, several GARCH variants, and autoregressive neural networks. In addition, preprocessing features have been added to allow arbitrary supervised machine learning models to be used in the context of forecasting.

To impliment this model we use \code{makeLearner()}, supplying the class of learner, order, the number of steps to forecast, and any additional arguments to be passed to \code{Arima} for \pkg{forecast}. 

<<makeArima, cache = TRUE>>=
tbatsMod =makeLearner("fcregr.tbats", use.box.cox = FALSE,
                      use.trend = TRUE,
                      seasonal.periods = TRUE, max.p = 3, max.q = 2,
                      stationary = FALSE, use.arma.errors = TRUE,
                      h = 35)
@

To train the model we simply call train, supplying the model and task.

<<makeArimaTrain, cache = TRUE>>=
trainTbats= train(tbatsMod, climate.task )
trainTbats
testp = predict(trainTbats, newdata = m4Test1)
performance(testp, mase, task = climate.task)
matplot(testp$data, type = "l")
@

While ARIMA is one of the most well known time series models, the order selection process can be subjective and difficult for users. One of the first proposals for automatic order selection comes from ~\cite{hannanOrder} where innovations are obtained by fitting high order autoregressive model to the data and then computing the likelihood of potential models through a series of standard regresssions. Proprietary algorithms from software such as \proglang{Forecast Pro} ~\cite{forecastpro} and \proglang{Autobox} ~\cite{reillyautobox} are well known and have performed to high standards in competitions such as the M3 forecasting competition ~\cite{Makridakis2000451}. One of the most well known R packages for automated forecast is \pkg{forecast} ~\cite{HyndForecast} which contains several methods for automated forecasting including exponential smoothing based methods and step-wise algorithms for forecasting with ARIMA models.

Forecasting in \pkg{mlr} takes a machine learning approach, creating a parameter set for a given model and using an optimization method to search over the parameter space. To do this, we will use a windowing resampling scheme to train over the possible models.

\section{Resampling with Time}

Resampling schemes such as cross-validation, bootstrapping, etc. are common in machine learning for dealing with the bias-variance tradeoff ~\cite{Friedman1997} ~\cite{rodriguezkfold}. When their is a time component to the data, windowing schemes are useful in allowing a valid resampling scheme while still maintaining the time properties of the series.\footnote{crap}. Figure one gives an example of what fixed and growing windows look like. Given a horizon and initial starting point the window slides forward one step each time while either shifting in the fixed case or enlarging by one in the growing case. Growing and fixed window resampling such as from ~\cite{hyndman2014forecasting} are now available in the \code{resampling()} function of \pkg{mlr}. 

\begin{figure}[ht]
\caption{Resampling with a window scheme as exampled by caret ~\cite{windowingcaret} }
  \includegraphics[scale = .2]{windowing_pic_caret}
  \centering
\end{figure}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.
\newpage

A windowing resampling process is created in the function \code{makeResampleDesc()} by supplying the resampling type, horizon, initial window, the length of the series, and an optional parameter to skip over some windows for the sake of time.

<<makeResampleDesc, cache = TRUE>>=
resampDesc = makeResampleDesc("GrowingCV", horizon = 35L,
                               initial.window = 400L,
                               size = nrow(getTaskData(climate.task)), skip = 6L)
resampDesc
@

To make a parameter set to tune over \pkg{mlr} uses \pkg{ParamHelpers} ~\cite{paramhelper}. For this example we use the random search procedure from ~\cite{Bergstra} to search our parameter space for the most optimal model.
<<makeparSet, eval = FALSE>>=
parSet = makeParamSet(
  makeLogicalParam(id = "use.box.cox",
                   default = FALSE,
                   tunable = TRUE),
  makeLogicalParam(id = "use.trend",
                   default = FALSE,
                   tunable = TRUE),
  makeLogicalParam(id = "use.damped.trend",
                   default = FALSE,
                   tunable = TRUE),
  makeLogicalParam(id = "seasonal.periods",
                   default = FALSE,
                   tunable = TRUE),
  makeIntegerParam(id = "max.p",
                   upper = 20,
                   lower = 0),
  makeIntegerParam(id = "start.p",
                   upper = 10,
                   lower = 1,
                   trafo = function(x) x*2),
  makeIntegerParam(id = "max.q",
                   upper = 20,
                   lower = 0),
  makeIntegerParam(id = "start.q",
                   upper = 10,
                   lower = 1,
                   trafo = function(x) x*2),
  makeIntegerParam("max.P",
                   lower = 0,
                   upper = 5),
  makeIntegerParam("max.Q",
                   lower = 0,
                   upper = 5),
  makeDiscreteParam("ic",
                    values = c("aicc","aic","bic")),
  makeDiscreteParam("test",
                    values = c("kpss","adf","pp")),
  makeDiscreteParam("seasonal.test",
                    values = c("ocsb", "ch")),
  makeLogicalParam("biasadj", default = FALSE),
  makeIntegerParam(id = "h",
                   default = 35,
                   tunable = FALSE,
                   lower = 35,
                   upper = 36)
)

#Specify tune by grid estimation
ctrl = makeTuneControlIrace(maxExperiments = 500L)
@

Using \code{tuneParams()} the model is tuned for the task using the specified resampling scheme, parameter set, tune control, and measure. For this tuning task we use MASE ~\cite{Hyndman2006} as a measure of performance \footnote{Models with a seasonal difference $> 0$ may be favorably biased as we use the non-seasonal MASE score}.

<<tuneparam, eval = FALSE>>=
#
library("parallelMap")
parallelStartSocket(6)
configureMlr(on.learner.error = "warn")
set.seed(1234)
tbatsTune = tuneParams("fcregr.tbats", task = climate.task, resampling = resampDesc,
                 par.set = parSet, control = ctrl, measures = mase)
parallelStop()
tbatsTune
@

<<loadTuneTbats,echo = FALSE, cache = TRUE>>=
load("./tbatsTune.RData")
tbatsTune
@


The best model's parameters are extracted using \code{setHyperPars()} and the best model is passed to \code{train()} to go over the full data set.

<<bestmodtrain, cache = TRUE>>=
lrn = setHyperPars(makeLearner("fcregr.tbats"), par.vals = tbatsTune$x)
m = train(lrn, climate.task)
@

To make predictions for our test set we simply pass our model, task, and test data to \code{predict()}

<<predbestArima, cache = TRUE>>=
climate.pred = predict(m, newdata = m4Test1)
performance(climate.pred, measures = mase, task = climate.task)
matplot(climate.pred$data, type = "l")
@

<<tuneGarch, eval = FALSE>>=
par_set = makeParamSet(
  makeDiscreteParam(id = "model", values = c("sGARCH", "csGARCH")),
  makeIntegerVectorParam(id = "garchOrder", len = 2L, lower = c(1,1), upper = c(4,4)),
  makeIntegerVectorParam(id = "armaOrder", len = 2L, lower = c(5,1), upper = c(8,3)),
  makeLogicalParam(id = "include.mean"),
  makeLogicalParam(id = "archm"),
  makeDiscreteParam(id = "distribution.model", values = c("norm","std","jsu")),
  makeDiscreteParam(id = "stationarity", c(0,1)),
  makeDiscreteParam(id = "fixed.se", c(0,1)),
  makeDiscreteParam(id = "solver", values = "nloptr"),
  makeIntegerParam(id = "n.ahead", default = 35L, lower = 35L,
                   upper = 36L, tunable = FALSE)
)

#Specify tune by grid estimation
ctrl = makeTuneControlIrace(maxExperiments = 400L)

parallelStartSocket(6)
configureMlr(on.learner.error = "warn")
set.seed(1234)
garchTune = tuneParams("fcregr.garch", task = climate.task, resampling = resampDesc,
                 par.set = par_set, control = ctrl, measures = mase)
parallelStop()
garchTune
@

<<loadGarchTune, cache = TRUE, echo = FALSE>>=
load("./garchTune.RData")
garchTune
@

<<trainGarchTuned, cache = TRUE, echo = FALSE>>=
lrn = setHyperPars(makeLearner("fcregr.garch", predict.type = "quantile"), par.vals = garchTune$x)
m = train(lrn, climate.task)
climate.pred = predict(m, newdata = m4Test1)
performance(climate.pred, measures = mase, task = climate.task)
@

<<garchPlot, cache = TRUE, echo = FALSE>>=
nn <- ncol(climate.pred$data)
par(xpd = TRUE)
matplot(climate.pred$data,type="l", xlab = "Forecast Horizon", ylab = "Temperature",
        main = "Forecast of Daily Climate Data\n With Confidence Intervals")
legend(-.5,-5.2, c("Truth", "Prediction", "Lower ConfInt .05", "Upper ConfInt .95"),col=seq_len(nn),cex=0.8,fill=seq_len(nn), ncol = 4, bty = "n")
@
\newpage
%\small
\bibliography{thesisbib}{}
\bibliographystyle{plain}

\end{document}