Forecasting in the MLR Framework
========================================================
author: Steve Bronder
date: October 11th, 2016
autosize: false

<style>
.small-code pre code {
  font-size: 1em;
}
</style>

Goal: Make Forecasting Simple
========================================================

> "We need to stop teaching abstinence and start teaching safe statistics"
- Hadley Wickham

Ex: Demeaning the whole data set before CV

- Forecasting is very dangerous
- Need a framework for 'safe forecasting'
- Want to use ML in forecasting

The Modeling Process
========================================================

<img src="modeling_process.jpeg" alt="Drawing" style="width: 1400px; height: 500px"/>

***

<img src="mlr_flow.jpeg" alt="Drawing" style="width: 2000px; height: 500px"/>

Example Data
======================================================
class: small-code 

```{r quandl, cache = TRUE, fig.align='center',fig.height= 12, fig.width= 15}
library(M4comp)
library(xts)
library(lubridate)
m4Fin1 <- M4[[8836]]
m4Train1 <- xts(m4Fin1$past, as.POSIXct("2007-12-05") + days(0:I(length(m4Fin1$past)-1)))
m4Test1 <- xts(m4Fin1$future, as.POSIXct("2009-09-06") + days(0:I(length(m4Fin1$future)-1)))
colnames(m4Train1) <- "target_var"
colnames(m4Test1) <- "target_var"

```

Plot of Climate Data
=====================================================
class: small-code 

```{r quandlPlot, cache = TRUE, fig.align='center',fig.height= 8, fig.width= 10, echo = FALSE}
plot(m4Train1, main = "Daily Climate Data")

```

Creating a Forecasting Task
========================================================
class: small-code 

```{r climateTask, cache = TRUE}
library(mlr)
climate.task = makeForecastRegrTask(id = "M4 Climate Data",data = m4Train1,
                                    target = "target_var",frequency = 365L)
climate.task
```

Making a Forecasting Learner
======================================================
class: small-code 

```{r arimaLearner, cache = TRUE}
tbatsMod =makeLearner("fcregr.tbats",
                      use.box.cox = FALSE,
                      use.trend = TRUE,
                      seasonal.periods = TRUE,
                      max.p = 3, max.q = 2,
                      stationary = FALSE,
                      use.arma.errors = TRUE, h = 35)
tbatsMod
```

Train a Forecast Learner
======================================================
class: small-code 

```{r trainArima, cache = TRUE}
trainTbats= train(tbatsMod, climate.task )
trainTbats

```

Predict With a Forecast Learner
======================================================
class: small-code 

```{r predictArima2, cache = TRUE}
testp = predict(trainTbats, newdata = m4Test1)
performance(testp, mase, task = climate.task)
mase
```

Prediction Plot
=====================================================
class: small-code 

```{r predplotArima, cache = TRUE, echo = FALSE, fig.align='center',fig.height= 8, fig.width= 10}
matplot(testp$data, type = "l", main = "Prediction Vs. Real Values\n For Model with No Tuning")
```


Tuning a Model
=====================================================
class: small-code 

```{r makeTuneParam, cache = TRUE}
parSet = makeParamSet(
  makeLogicalParam(id = "use.box.cox", default = FALSE, tunable = TRUE),
  makeLogicalParam(id = "use.trend", default = FALSE, tunable = TRUE),
  makeLogicalParam(id = "use.damped.trend", default = FALSE, tunable = TRUE),
  makeLogicalParam(id = "seasonal.periods", default = FALSE, tunable = TRUE),
  makeIntegerParam(id = "max.p", upper = 20, lower = 0),
  makeIntegerParam(id = "start.p", upper = 10, lower = 1, 
                   trafo = function(x) x*2),
  makeIntegerParam(id = "max.q", upper = 20, lower = 0),
  makeIntegerParam(id = "start.q", upper = 10,lower = 1,
                   trafo = function(x) x*2),
  makeIntegerParam("max.P", lower = 0, upper = 5),
  makeIntegerParam("max.Q", lower = 0, upper = 5),
  makeDiscreteParam("ic", values = c("aicc","aic","bic")),
  makeDiscreteParam("test", values = c("kpss","adf","pp")),
  makeDiscreteParam("seasonal.test", values = c("ocsb", "ch")),
  makeLogicalParam("biasadj", default = FALSE)
  )
```


Make a Tune Control Scheme
=====================================================
class: small-code 

```{r makeTuneControl, cache = TRUE}
#Specify tune by grid estimation
ctrl = makeTuneControlIrace(maxExperiments = 500L)
ctrl
```

Making a Resample Scheme
========================================
class: small-code 

```{r resampleScheme, cache = TRUE}
resampDesc = makeResampleDesc("GrowingCV", horizon = 35L,
                               initial.window = 400L,
                               size = nrow(getTaskData(climate.task)), skip = 6L)
resampDesc
```

Example of Windowing Resample
======================================================
<center>
<img src="caret_window.png" alt="Drawing" style="width: 1200px; height: 600px"/>
</center>

Tuning Over Parameter Space
===========================================
class: small-code 

```{r tuneSpace, eval = FALSE}
library("parallelMap")
parallelStartSocket(6)
tbatsTune = tuneParams(makeLearner("fcregr.tbats",h = 35), 
                       task = climate.task,
                       resampling = resampDesc,
                       par.set = parSet,
                       control = ctrl,
                       measures = mase)
parallelStop()
tbatsTune
```

```{r loadTune, echo = FALSE}
load("./../tbatsTune.RData")
tbatsTune
```


Training Best Model
=========================================
class: small-code 

```{r runBest, cache = TRUE}
lrn = setHyperPars(makeLearner("fcregr.tbats"), par.vals = tbatsTune$x)
m = train(lrn, climate.task)
m
```

Prediction With Best Model
=========================================
class: small-code 

```{r predBest, cache = TRUE}
climate.pred = predict(m, newdata = m4Test1)
performance(climate.pred, measures = mase, task = climate.task)
```

Prediction Plot With Best Model
=========================================
class: small-code 

```{r predplottbats, cache = TRUE, echo = FALSE, fig.align='center',fig.height= 8, fig.width= 10}

matplot(climate.pred$data, type = "l", main = "Prediction Vs. Real Data \n With Tuned Model")

```

Using an ML Model: Making Lagged Tasks
=========================================
class: small-code 

```{r mlLagTask, cache = TRUE}
climateLagTask = createLagDiffFeatures(climate.task, lag = 35L:100L, na.pad = FALSE, return.nonlag = TRUE)
climateLagTask$env$data <- climateLagTask$env$data[,-1]
climateLagTask$task.desc$type <- "regr"
climateLagTask
```

Using an ML Model: Making Lagged Tasks
=========================================
class: small-code 

```{r mlLagTrain, cache = TRUE}
regrGbm = makeLearner("regr.gbm",
                      par.vals = list(n.trees = 2000,
                                      interaction.depth = 8,
                                      distribution = "laplace"))
gbmMod = train(regrGbm, climateLagTask)
```

Make Resampling Scheme
===========================================
class: small-code 

```{r resampleSchemegbm, cache = TRUE}
resampDesc = makeResampleDesc("GrowingCV", horizon = 35L,
                               initial.window = 300L,
                               size = nrow(getTaskData(climateLagTask)),
                               skip = 6L)
resampDesc
```

Make Tuning Set and Search Scheme
===========================================
class: small-code 

```{r gbmtuning, cache = TRUE}
ps = makeParamSet(
  makeDiscreteParam("distribution", values = c("laplace", "tdist")),
  #
  makeIntegerParam("n.trees",lower = 1, upper = 3,
                   trafo = function(x) x * 100),
  #
  makeIntegerParam("interaction.depth", lower = 1, upper = 3,
                   trafo = function(x) x * 5),
  #
  makeNumericParam("shrinkage", lower = 1, upper = 2)
)
ctrl = makeTuneControlIrace(maxExperiments = 300L)
```

Tuning GBM Model
===========================================
class: small-code 

```{r tuneSpacegbm, cache = TRUE, eval = FALSE}
library("parallelMap")
parallelStartSocket(6)
configureMlr(on.learner.error = "warn")
set.seed(1234)
res = tuneParams("regr.gbm", task = climateLagTask,
                 resampling = resampDesc,
                 par.set = ps, control = ctrl,
                 measures = mase)
parallelStop()
res
```

```{r loadgbmTune, echo = FALSE, cache = TRUE}
load("./gbmTune.RData")
res
```


Tuning GBM Model
===========================================
class: small-code 

```{r fitgbm, cache = TRUE, eval = TRUE}
lrn = setHyperPars(makeLearner("regr.gbm"), par.vals = res$x)
m = train(lrn, climateLagTask)
m
```
Forecast with ML Model
======================================================
class: small-code 

```{r mlPred, cache = TRUE, eval = TRUE}
m4Full <- rbind(m4Train1,m4Test1)
climateLagData = createLagDiffFeatures(m4Full, lag = 35L:100L,
                                       na.pad = FALSE,
                                       return.nonlag = TRUE)
climateLagTest = climateLagData[I(nrow(climateLagData) - 
                                    35):I(nrow(climateLagData)),]
testp <- predict(gbmMod, newdata = climateLagTest)
performance(testp, mase, climateLagTask)
```

Plot Forecast
===========================================
class: small-code 
```{r mlPredictGraph, eval = TRUE, cache = TRUE, fig.align='center', fig.height= 8, fig.width= 10, echo = FALSE}
nn <- ncol(testp$data)
par(xpd = TRUE)
matplot(testp$data,type="l", xlab = "Forecast Horizon", ylab = "Temperature",
        main = "Forecast of Daily Climate Data\n With Confidence Intervals")
legend(10,15, c("Truth", "Prediction"),col=seq_len(nn),cex=0.8,fill=seq_len(nn), ncol = 4, bty = "n")
```

