Forecasting in the MLR Framework
========================================================
author: Steve Bronder
date: October 11th, 2016
autosize: true

What is Forecasting?
========================================================

- Predictions of future based on past trends
- What happens Vs. What happens tomorrow
- Stocks, earthquakes, neuroscience

Goal: Make Forecasting Simple
========================================================

> "We need to stop teaching abstinence and start teaching safe statistics"
- Hadley Wickham

Ex: Demeaning the whole data set before CV

- Problem: Forecasting can be dangerous
- Insight: Need a framework for 'safe forecasting'
- Solution: Use ML Framework in forecasting

The Modeling Process
========================================================

<img src="flownames.jpeg" alt="Drawing" style="width: 90%; height: 90%"/>

***

- MLR automates this pipeline
- Make forecasting safer by using the pipeline

Example Data
======================================================

```{r quandl, cache = TRUE, fig.align='center'}
library(Quandl)
library(xts)
aapl <- Quandl("YAHOO/AAPL", api_key="UG7wmFCm6zMyq1xhW9Re")
aaplXts <- xts(aapl$Close, order.by = as.POSIXlt(aapl$Date))
colnames(aaplXts) <- "Close"
aaplXtsTrain <- aaplXts[1:9000,]
aaplXtsTest  <- aaplXts[9001:9035,]

```

Plot of aapl Stock
=====================================================

```{r quandlPlot, cache = TRUE, eval = FALSE, echo = FALSE}
plot(aaplXtsTrain, main = "Training: Closing Price of Apple by Day")
plot(aaplXtsTest, main = "Testing: Closing Price of Apple by Day")
```

<img src="training_aapl.png" alt="Drawing" style="width: 90%; height: 90%"/>

***

<img src="testing_aapl.png" alt="Drawing" style="width: 90%; height: 90%"/>

Creating a Forecasting Task
========================================================

- Task: Keeps data and meta-data for ML task

```{r aaplTask, cache = TRUE}
library(mlr)
aaplTask <- makeForecastRegrTask(
  id = "Forecast aapl Closing Price",
  data = aaplXtsTrain,
  target  = "Close",
  frequency = 7L)
```

Creating a Forecasting Task: Info
========================================================
```{r aaplTaskView, cache = TRUE}
aaplTask
```


Making a Forecasting Learner
======================================================

```{r arimaLearner, cache = TRUE}
garch.mod =makeLearner("fcregr.garch", 
                      model = "sGARCH",
                      garchOrder = c(5,5),
                      distribution.model = "sged",
                      armaOrder = c(6,6),
                      n.ahead = 35,
                      predict.type = "quantile")

```

Making a Forecasting Learner
======================================================

```{r arimaLearnerView, cache = TRUE}
garch.mod 
```

Train a Forecast Learner
======================================================

```{r trainArima, cache = TRUE,eval = FALSE}
garch.train <- train(garch.mod, aaplTask)
garch.train
```

Predict With a Forecast Learner
======================================================
```{r predictArima2, cache = TRUE}
predAapl <- predict(garch.train, newdata = as.data.frame(aaplXtsTest))
performance(predAapl, measures = mase, task = aaplTask)
```

Prediction Plot
=====================================================

```{r predplotArima, cache = TRUE, echo = FALSE, fig.align='center',fig.height= 8, fig.width= 10}
matplot(predAapl$data, type = "l", main = "Prediction Vs. Real Values of Next 35 Days")
```

```{r updateModelArima, cache = TRUE, echo = FALSE, eval = FALSE}
tbatsUpdate  = updateModel(train.tbats, aaplTask, newdata = aaplXtsTest)
updatePred = predict(tbatsUpdate, task = aaplTask)
updatePred
```

Tuning a Model
=====================================================

```{r makeTuneParam, cache = TRUE}
# Make a tuning grid for GARCH
par_set = makeParamSet(
  makeDiscreteParam(id = "model",
                    values = c("sGARCH", "csGARCH", "fGARCH")),
  makeDiscreteParam("submodel", values = c("GARCH","TGARCH","AVGARCH"),requires = quote(model == 'fGARCH') ),
  makeIntegerVectorParam(id = "garchOrder", len = 2L,
                         lower = 1, upper = 8),
  makeIntegerVectorParam(id = "armaOrder", len = 2L,
                         lower = 1, upper = 9),
  makeLogicalParam(id = "include.mean"),
  makeLogicalParam(id = "archm"),
  makeDiscreteParam(id = "distribution.model",
                    values = c("norm","std","jsu", "sged")),
  makeDiscreteParam(id = "stationarity", c(0,1)),
  makeDiscreteParam(id = "fixed.se", c(0,1))
)
```

Making a Resample Scheme
========================================

```{r resampleScheme, cache = TRUE}
resampDesc = makeResampleDesc("GrowingCV", horizon = 35L,
                              initial.window = .9,
                              size = nrow(getTaskData(aaplTask)),
                              skip = .01)
resampDesc
```

Example of Windowing Resample
======================================================
<center>
<img src="caret_window.png" alt="Drawing" style="width: 1200px; height: 600px"/>
</center>

Making a Tuning Control
===========================================

```{r ctrlmake, cache = TRUE}
ctrl <- makeTuneControlIrace(maxExperiments = 350)
```

Tuning Over Parameter Space
===========================================

```{r tuneSpace, eval = FALSE}
library("parallelMap")
parallelStart("multicore",3)
configureMlr(on.learner.error = "warn")
set.seed(1234)

garch.mod = makeLearner("fcregr.garch", n.ahead = 35, solver = 'hybrid')
garch.res = tuneParams(garch.mod, task = aaplTask,
                 resampling = resampDesc, par.set = par_set,
                 control = ctrl,
                 measures = mase)
parallelStop()

garch.final = setHyperPars(makeLearner("fcregr.garch", n.ahead = 35, solver = 'nloptr', solver.control = list(maxeval = 200000, solver = 10), predict.type = "quantile"),par.vals = garch.res$x)

garch.train = train(garch.final, aaplTask)
garch.pred = predict(garch.train, newdata = aaplXtsTest)
performance(garch.pred, measures = mase, task = aaplTask)
matplot(garch.pred$data, type = "l")
save(garch.res, file ="./garch_tune_mod.RData")
```


Using an ML Model
=========================================

```{r mlLagTask, cache = TRUE, eval = FALSE}
aaplRegTask <- makeRegrTask(
  id = "Forecast aapl Closing Price",
  data = as.data.frame(aaplXtsTrain,rownames = index(aaplXtsTrain)),
  target  = "Close")

aaplLagTask = createLagDiffFeatures(aaplRegTask, lag = 1L:665L, difference = 1L, na.pad = FALSE)


## Trying Support Vector Machines
xg_learner <- makeLearner("regr.xgboost")

xg_param_set <- makeParamSet(
  makeDiscreteParam(id = "booster", values = c("gbtree")),
  makeNumericLearnerParam(id = "eta", lower = 0, upper = 1),
  makeNumericLearnerParam(id = "gamma", lower = 0, upper = 100),
  makeIntegerLearnerParam(id = "max_depth",  lower = 100L, upper = 300),
  makeIntegerLearnerParam(id = "nrounds", lower = 100L, upper = 300)
)
# A pretty good model had something like
# [Tune-x] 1: booster=gbtree; eta=0.248; gamma=61.4; max_depth=425; subsample=0.456;
# colsample_bytree=0.971; colsample_bylevel=0.612; lambda=18.5; lambda_bias=9.49;
# alpha=5.62; nrounds=493

ctrl <- makeTuneControlIrace(maxExperiments = 200)

library("parallelMap")
parallelStart("multicore",3)
configureMlr(on.learner.error = "warn")
tune_mod <- tuneParams(learner = xg_learner, task = aaplLagTask,
                       measures = mase, resampling = resampDesc,
                       par.set = xg_param_set, control = ctrl )
parallelStop()

gbm_final = setHyperPars(xg_learner, par.vals = tune_mod$x)
gbm_train <- train(gbm_final, aaplLagTask)
gbm_fore = forecast(gbm_train, h = 35, newdata = aaplXtsTest)
performance(gbm_fore, mase, task = aaplLagTask)
save(tune_mod, file = "./gbm_tune_mod.RData")
```

Make Resampling Scheme
===========================================

```{r resampleSchemegbm, cache = TRUE}
resampDesc = makeResampleDesc("GrowingCV", horizon = 35L,
                               initialWindow = 7000L,
                               size = nrow(getTaskData(aaplLagTask)), skip = 35L)
```

Make Tuning Set and Search Scheme
===========================================
```{r gbmtuning}
library(mlr)
ps = makeParamSet(
  makeDiscreteParam("distribution", values = c("gaussian","laplace", "tdist")),
  makeIntegerParam("n.trees", lower = 1, upper = 10, trafo = function(x) x * 1000),
  makeIntegerParam("interaction.depth", lower = 1, upper = 5, trafo = function(x) x * 5),
  makeNumericParam("shrinkage", lower = 1E-5, upper = 1E-2)
)
ctrl = makeTuneControlIrace(maxExperiments = 200L)
```
Tuning GBM Model
===========================================

```{r tuneSpacegbm, cache = TRUE}
library("parallelMap")
parallelStartSocket(3)
configureMlr(on.learner.error = "warn")
set.seed(1234)
res = tuneParams("regr.gbm", task = aaplLagTask, resampling = resampDesc,
                 par.set = ps, control = ctrl, measures = mse)
parallelStop()

```

Predict with ML Model
======================================================
```{r mlPred, cache = TRUE}
testp <- predict(gbmMod, newdata = aaplLagTest)
performance(testp, mase, aaplLagTask)
```

Plot Prediction
===========================================
```{r mlPredictGraph, cache = TRUE, fig.align='center', fig.height= 8, fig.width= 10, echo = FALSE}
testXts <- xts(testp$data, as.POSIXlt(rownames(testp$data)))
testZoo <- as.zoo(testXts)
tsRainbow <- rainbow(ncol(testZoo))
plot(x = testZoo, ylab = "Closing Price", main = "Prediction Vs. Real",
col = tsRainbow, screens = 1)
```


Make Forecast
===========================================

```{r mlforecast, cache = TRUE, fig.align='center', fig.height= 8, fig.width= 10, echo = TRUE}
library(lubridate)
aaplLagFore = createLagDiffFeatures(aaplXts, lag = 0L:65L, na.pad = FALSE, return.nonlag = FALSE)
aaplLagFore = aaplLagFore[I(nrow(aaplLagFore) - 35L):nrow(aaplLagFore),]
colnames(aaplLagFore)[1:66] = colnames(aaplLagTrain)[2:67]
forep = predict(gbmMod, newdata = aaplLagFore)
```

Plot Forecast
===========================================

```{r mlforecastPlot, cache = TRUE, fig.align='center', fig.height= 8, fig.width= 10, echo = FALSE}
testXts = xts(forep$data, as.POSIXlt(rownames(forep$data)) + days(53))
testZoo = as.zoo(testXts)
plot(x = testZoo, ylab = "Closing Price", main = "Forecast of aapl Closing Stock Price")
```
